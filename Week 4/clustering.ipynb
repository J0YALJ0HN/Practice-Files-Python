{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "tags": [],
        "id": "5P8B2tzH1cBa"
      },
      "source": [
        "# Setup"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0_wqF0_31cBa"
      },
      "source": [
        "This project requires Python 3.7 or above:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dm0Gg3141cBa"
      },
      "outputs": [],
      "source": [
        "import sys\n",
        "\n",
        "assert sys.version_info >= (3, 7)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gL9VVOkz1cBb"
      },
      "source": [
        "It also requires Scikit-Learn ≥ 1.0.1:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WzYaniCX1cBb"
      },
      "outputs": [],
      "source": [
        "from packaging import version\n",
        "import sklearn\n",
        "\n",
        "assert version.parse(sklearn.__version__) >= version.parse(\"1.0.1\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bHJq_K_L1cBb"
      },
      "source": [
        "As we did in previous chapters, let's define the default font sizes to make the figures prettier:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gtHT7K0U1cBc"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "plt.rc('font', size=14)\n",
        "plt.rc('axes', labelsize=14, titlesize=14)\n",
        "plt.rc('legend', fontsize=14)\n",
        "plt.rc('xtick', labelsize=10)\n",
        "plt.rc('ytick', labelsize=10)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N-jyQdwD1cBd"
      },
      "source": [
        "And let's create the `images/unsupervised_learning` folder (if it doesn't already exist), and define the `save_fig()` function which is used through this notebook to save the figures in high-res for the book:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MbBT7XGN1cBd"
      },
      "outputs": [],
      "source": [
        "from pathlib import Path\n",
        "\n",
        "IMAGES_PATH = Path() / \"images\" / \"unsupervised_learning\"\n",
        "IMAGES_PATH.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "def save_fig(fig_id, tight_layout=True, fig_extension=\"png\", resolution=300):\n",
        "    path = IMAGES_PATH / f\"{fig_id}.{fig_extension}\"\n",
        "    if tight_layout:\n",
        "        plt.tight_layout()\n",
        "    plt.savefig(path, format=fig_extension, dpi=resolution)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np"
      ],
      "metadata": {
        "id": "wjE6WiYYA0Jw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H2ehAJEl1cBd"
      },
      "source": [
        "First, let's import a few common modules, ensure MatplotLib plots figures inline and prepare a function to save the figures."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1uYywZcJ1cBd"
      },
      "source": [
        "# Clustering"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HSgwJCAq1cBd"
      },
      "source": [
        "**Introduction – Classification _vs_ Clustering**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xCP0ujEq1cBd"
      },
      "outputs": [],
      "source": [
        "# extra code – this cell generates and saves Figure 9–1\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.datasets import load_iris\n",
        "\n",
        "data = load_iris()\n",
        "X = data.data\n",
        "y = data.target\n",
        "data.target_names\n",
        "\n",
        "plt.figure(figsize=(9, 3.5))\n",
        "\n",
        "plt.subplot(121)\n",
        "plt.plot(X[y==0, 2], X[y==0, 3], \"yo\", label=\"Iris setosa\")\n",
        "plt.plot(X[y==1, 2], X[y==1, 3], \"bs\", label=\"Iris versicolor\")\n",
        "plt.plot(X[y==2, 2], X[y==2, 3], \"g^\", label=\"Iris virginica\")\n",
        "plt.xlabel(\"Petal length\")\n",
        "plt.ylabel(\"Petal width\")\n",
        "plt.grid()\n",
        "plt.legend()\n",
        "\n",
        "plt.subplot(122)\n",
        "plt.scatter(X[:, 2], X[:, 3], c=\"k\", marker=\".\")\n",
        "plt.xlabel(\"Petal length\")\n",
        "plt.tick_params(labelleft=False)\n",
        "plt.gca().set_axisbelow(True)\n",
        "plt.grid()\n",
        "\n",
        "save_fig(\"classification_vs_clustering_plot\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qsVAjp9m1cBe"
      },
      "source": [
        "What's the ratio of iris plants we assigned to the right cluster?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WBjBzSRY1cBf"
      },
      "source": [
        "## K-Means"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DpSysVz11cBf"
      },
      "source": [
        "**Fit and predict**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TEpeiSYy1cBf"
      },
      "source": [
        "Let's train a K-Means clusterer on a dataset if blobs. It will try to find each blob's center and assign each instance to the closest blob:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0oR2IPFH1cBg"
      },
      "outputs": [],
      "source": [
        "from sklearn.cluster import KMeans\n",
        "from sklearn.datasets import make_blobs\n",
        "\n",
        "# extra code – the exact arguments of make_blobs() are not important\n",
        "blob_centers = np.array([[ 0.2,  2.3], [-1.5 ,  2.3], [-2.8,  1.8],\n",
        "                         [-2.8,  2.8], [-2.8,  1.3]])\n",
        "blob_std = np.array([0.4, 0.3, 0.1, 0.1, 0.1])\n",
        "X, y = make_blobs(n_samples=2000, centers=blob_centers, cluster_std=blob_std,\n",
        "                  random_state=7)\n",
        "\n",
        "k = 5\n",
        "kmeans = KMeans(n_clusters=k, random_state=42)\n",
        "y_pred = kmeans.fit_predict(X)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Syi3c6QR1cBg"
      },
      "source": [
        "Now let's plot them:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZLY2t-uY1cBg"
      },
      "outputs": [],
      "source": [
        "# extra code – this cell generates and saves Figure 9–2\n",
        "\n",
        "def plot_clusters(X, y=None):\n",
        "    plt.scatter(X[:, 0], X[:, 1], c=y, s=1)\n",
        "    plt.xlabel(\"$x_1$\")\n",
        "    plt.ylabel(\"$x_2$\", rotation=0)\n",
        "\n",
        "plt.figure(figsize=(8, 4))\n",
        "plot_clusters(X)\n",
        "plt.gca().set_axisbelow(True)\n",
        "plt.grid()\n",
        "save_fig(\"blobs_plot\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lZMrfBKb1cBg"
      },
      "source": [
        "Each instance was assigned to one of the 5 clusters:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bAsnvu641cBg"
      },
      "outputs": [],
      "source": [
        "y_pred"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "S1ZVFX2J1cBh"
      },
      "outputs": [],
      "source": [
        "y_pred is kmeans.labels_"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IaEu0dTW1cBh"
      },
      "source": [
        "And the following 5 _centroids_ (i.e., cluster centers) were estimated:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "t5I0dv0J1cBh"
      },
      "outputs": [],
      "source": [
        "kmeans.cluster_centers_"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ko8NpJ5F1cBh"
      },
      "source": [
        "Note that the `KMeans` instance preserves the labels of the instances it was trained on. Somewhat confusingly, in this context, the _label_ of an instance is the index of the cluster that instance gets assigned to (they are not targets, they are predictions):"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "veI-nGQ_1cBh"
      },
      "outputs": [],
      "source": [
        "kmeans.labels_"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s_X9hhNk1cBh"
      },
      "source": [
        "Of course, we can predict the labels of new instances:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sS2DjPUu1cBh"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "X_new = np.array([[0, 2], [3, 2], [-3, 3], [-3, 2.5]])\n",
        "kmeans.predict(X_new)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PsyksK7f1cBi"
      },
      "source": [
        "**Decision Boundaries**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7eJtNTDd1cBi"
      },
      "source": [
        "Let's plot the model's decision boundaries. This gives us a _Voronoi diagram_:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0YNxbos01cBi"
      },
      "outputs": [],
      "source": [
        "# extra code – this cell generates and saves Figure 9–3\n",
        "\n",
        "def plot_data(X):\n",
        "    plt.plot(X[:, 0], X[:, 1], 'k.', markersize=2)\n",
        "\n",
        "def plot_centroids(centroids, weights=None, circle_color='w', cross_color='k'):\n",
        "    if weights is not None:\n",
        "        centroids = centroids[weights > weights.max() / 10]\n",
        "    plt.scatter(centroids[:, 0], centroids[:, 1],\n",
        "                marker='o', s=35, linewidths=8,\n",
        "                color=circle_color, zorder=10, alpha=0.9)\n",
        "    plt.scatter(centroids[:, 0], centroids[:, 1],\n",
        "                marker='x', s=2, linewidths=12,\n",
        "                color=cross_color, zorder=11, alpha=1)\n",
        "\n",
        "def plot_decision_boundaries(clusterer, X, resolution=1000, show_centroids=True,\n",
        "                             show_xlabels=True, show_ylabels=True):\n",
        "    mins = X.min(axis=0) - 0.1\n",
        "    maxs = X.max(axis=0) + 0.1\n",
        "    xx, yy = np.meshgrid(np.linspace(mins[0], maxs[0], resolution),\n",
        "                         np.linspace(mins[1], maxs[1], resolution))\n",
        "    Z = clusterer.predict(np.c_[xx.ravel(), yy.ravel()])\n",
        "    Z = Z.reshape(xx.shape)\n",
        "\n",
        "    plt.contourf(Z, extent=(mins[0], maxs[0], mins[1], maxs[1]),\n",
        "                cmap=\"Pastel2\")\n",
        "    plt.contour(Z, extent=(mins[0], maxs[0], mins[1], maxs[1]),\n",
        "                linewidths=1, colors='k')\n",
        "    plot_data(X)\n",
        "    if show_centroids:\n",
        "        plot_centroids(clusterer.cluster_centers_)\n",
        "\n",
        "    if show_xlabels:\n",
        "        plt.xlabel(\"$x_1$\")\n",
        "    else:\n",
        "        plt.tick_params(labelbottom=False)\n",
        "    if show_ylabels:\n",
        "        plt.ylabel(\"$x_2$\", rotation=0)\n",
        "    else:\n",
        "        plt.tick_params(labelleft=False)\n",
        "\n",
        "plt.figure(figsize=(8, 4))\n",
        "plot_decision_boundaries(kmeans, X)\n",
        "save_fig(\"voronoi_plot\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hSg4kxRI1cBi"
      },
      "source": [
        "Not bad! Some of the instances near the edges were probably assigned to the wrong cluster, but overall it looks pretty good."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aI_YkK951cBr"
      },
      "source": [
        "**Hard Clustering _vs_ Soft Clustering**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8T4PX1Wi1cBr"
      },
      "source": [
        "Rather than arbitrarily choosing the closest cluster for each instance, which is called _hard clustering_, it might be better to measure the distance of each instance to all 5 centroids. This is what the `transform()` method does:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "i7nEVvzR1cBr"
      },
      "outputs": [],
      "source": [
        "kmeans.transform(X_new).round(2)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wL4hI6q61cBu"
      },
      "source": [
        "### Inertia"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kHJdW9Ao1cBu"
      },
      "source": [
        "To select the best model, we will need a way to evaluate a K-Mean model's performance. Unfortunately, clustering is an unsupervised task, so we do not have the targets. But at least we can measure the distance between each instance and its centroid. This is the idea behind the _inertia_ metric:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "W6BiGJOD1cBu"
      },
      "outputs": [],
      "source": [
        "kmeans.inertia_"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F-u4LxcX1cBv"
      },
      "source": [
        "### Multiple Initializations"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LtKRDBaE1cBv"
      },
      "source": [
        "So one approach to solve the variability issue is to simply run the K-Means algorithm multiple times with different random initializations, and select the solution that minimizes the inertia."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AqibgkKm1cBv"
      },
      "source": [
        "When you set the `n_init` hyperparameter, Scikit-Learn runs the original algorithm `n_init` times, and selects the solution that minimizes the inertia. By default, Scikit-Learn sets `n_init=10`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "86A9xju11cBv"
      },
      "outputs": [],
      "source": [
        "# extra code\n",
        "kmeans_rnd_10_inits = KMeans(n_clusters=5, init=\"random\", n_init=10,\n",
        "                             random_state=2)\n",
        "kmeans_rnd_10_inits.fit(X)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jhYNDACB1cBv"
      },
      "source": [
        "As you can see, we end up with the initial model, which is certainly the optimal K-Means solution (at least in terms of inertia, and assuming $k=5$)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pNeJ2K-m1cBv"
      },
      "outputs": [],
      "source": [
        "# extra code\n",
        "plt.figure(figsize=(8, 4))\n",
        "plot_decision_boundaries(kmeans_rnd_10_inits, X)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nCqsZ_Ou1cBw"
      },
      "outputs": [],
      "source": [
        "kmeans_rnd_10_inits.inertia_"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-Cyv7tjG1cBz"
      },
      "source": [
        "### Finding the optimal number of clusters"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uYIeeNcDCaxg"
      },
      "outputs": [],
      "source": [
        "# extra code – this cell generates and saves Figure 9–5\n",
        "\n",
        "def plot_clusterer_comparison(clusterer1, clusterer2, X, title1=None,\n",
        "                              title2=None):\n",
        "    clusterer1.fit(X)\n",
        "    clusterer2.fit(X)\n",
        "\n",
        "    plt.figure(figsize=(10, 3.2))\n",
        "\n",
        "    plt.subplot(121)\n",
        "    plot_decision_boundaries(clusterer1, X)\n",
        "    if title1:\n",
        "        plt.title(title1)\n",
        "\n",
        "    plt.subplot(122)\n",
        "    plot_decision_boundaries(clusterer2, X, show_ylabels=False)\n",
        "    if title2:\n",
        "        plt.title(title2)\n",
        "\n",
        "kmeans_rnd_init1 = KMeans(n_clusters=5, init=\"random\", n_init=1, random_state=2)\n",
        "kmeans_rnd_init2 = KMeans(n_clusters=5, init=\"random\", n_init=1, random_state=9)\n",
        "\n",
        "plot_clusterer_comparison(kmeans_rnd_init1, kmeans_rnd_init2, X,\n",
        "                          \"Solution 1\",\n",
        "                          \"Solution 2 (with a different random init)\")\n",
        "\n",
        "save_fig(\"kmeans_variability_plot\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nJhvGB671cBz"
      },
      "source": [
        "What if the number of clusters was set to a lower or greater value than 5?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WPZNSgD11cBz"
      },
      "outputs": [],
      "source": [
        "# extra code – this cell generates and saves Figure 9–7\n",
        "\n",
        "kmeans_k3 = KMeans(n_clusters=3, random_state=42)\n",
        "kmeans_k8 = KMeans(n_clusters=8, random_state=42)\n",
        "\n",
        "plot_clusterer_comparison(kmeans_k3, kmeans_k8, X, \"$k=3$\", \"$k=8$\")\n",
        "save_fig(\"bad_n_clusters_plot\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fOKjGrMT1cBz"
      },
      "source": [
        "Ouch, these two models don't look great. What about their inertias?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bue0fcky1cBz"
      },
      "outputs": [],
      "source": [
        "kmeans_k3.inertia_"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BxRZ20zh1cB0"
      },
      "outputs": [],
      "source": [
        "kmeans_k8.inertia_"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wz2v-hpf1cB0"
      },
      "source": [
        "No, we cannot simply take the value of $k$ that minimizes the inertia, since it keeps getting lower as we increase $k$. Indeed, the more clusters there are, the closer each instance will be to its closest centroid, and therefore the lower the inertia will be. However, we can plot the inertia as a function of $k$ and analyze the resulting curve:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "I7OhN5aP1cB0"
      },
      "outputs": [],
      "source": [
        "# extra code – this cell generates and saves Figure 9–8\n",
        "\n",
        "kmeans_per_k = [KMeans(n_clusters=k, random_state=42).fit(X)\n",
        "                for k in range(1, 10)]\n",
        "inertias = [model.inertia_ for model in kmeans_per_k]\n",
        "\n",
        "plt.figure(figsize=(8, 3.5))\n",
        "plt.plot(range(1, 10), inertias, \"bo-\")\n",
        "plt.xlabel(\"$k$\")\n",
        "plt.ylabel(\"Inertia\")\n",
        "plt.annotate(\"\", xy=(4, inertias[3]), xytext=(4.45, 650),\n",
        "             arrowprops=dict(facecolor='black', shrink=0.1))\n",
        "plt.text(4.5, 650, \"Elbow\", horizontalalignment=\"center\")\n",
        "plt.axis([1, 8.5, 0, 1300])\n",
        "plt.grid()\n",
        "save_fig(\"inertia_vs_k_plot\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dmT_Yhau1cB0"
      },
      "source": [
        "As you can see, there is an elbow at $k=4$, which means that less clusters than that would be bad, and more clusters would not help much and might cut clusters in half. So $k=4$ is a pretty good choice. Of course in this example it is not perfect since it means that the two blobs in the lower left will be considered as just a single cluster, but it's a pretty good clustering nonetheless."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tfYl3aPa1cB1"
      },
      "source": [
        "Another approach is to look at the _silhouette score_, which is the mean _silhouette coefficient_ over all the instances. An instance's silhouette coefficient is equal to (_b_ - _a_) / max(_a_, _b_) where _a_ is the mean distance to the other instances in the same cluster (it is the _mean intra-cluster distance_), and _b_ is the _mean nearest-cluster distance_, that is the mean distance to the instances of the next closest cluster (defined as the one that minimizes _b_, excluding the instance's own cluster). The silhouette coefficient can vary between -1 and +1: a coefficient close to +1 means that the instance is well inside its own cluster and far from other clusters, while a coefficient close to 0 means that it is close to a cluster boundary, and finally a coefficient close to -1 means that the instance may have been assigned to the wrong cluster."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pue8qUKc1cB1"
      },
      "source": [
        "Let's plot the silhouette score as a function of $k$:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nXzCHmkd1cB1"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import silhouette_score"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Hqf7CcR21cB1"
      },
      "outputs": [],
      "source": [
        "silhouette_score(X, kmeans.labels_)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Q6kPkTTb1cB1"
      },
      "outputs": [],
      "source": [
        "# extra code – this cell generates and saves Figure 9–9\n",
        "\n",
        "silhouette_scores = [silhouette_score(X, model.labels_)\n",
        "                     for model in kmeans_per_k[1:]]\n",
        "\n",
        "plt.figure(figsize=(8, 3))\n",
        "plt.plot(range(2, 10), silhouette_scores, \"bo-\")\n",
        "plt.xlabel(\"$k$\")\n",
        "plt.ylabel(\"Silhouette score\")\n",
        "plt.axis([1.8, 8.5, 0.55, 0.7])\n",
        "plt.grid()\n",
        "save_fig(\"silhouette_score_vs_k_plot\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fRTb9AZL1cB1"
      },
      "source": [
        "As you can see, this visualization is much richer than the previous one: in particular, although it confirms that $k=4$ is a very good choice, but it also underlines the fact that $k=5$ is quite good as well."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dR5W2Cx71cCL"
      },
      "source": [
        "## 10. Cluster the Olivetti Faces Dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NCqt4xV91cCL"
      },
      "source": [
        "*Exercise: The classic Olivetti faces dataset contains 400 grayscale 64 × 64–pixel images of faces. Each image is flattened to a 1D vector of size 4,096. 40 different people were photographed (10 times each), and the usual task is to train a model that can predict which person is represented in each picture. Load the dataset using the `sklearn.datasets.fetch_olivetti_faces()` function.*"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Sn09PBZ61cCL"
      },
      "outputs": [],
      "source": [
        "from sklearn.datasets import fetch_olivetti_faces\n",
        "\n",
        "olivetti = fetch_olivetti_faces()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1SMo3j7r1cCL"
      },
      "outputs": [],
      "source": [
        "print(olivetti.DESCR)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yssOKYHw1cCL"
      },
      "outputs": [],
      "source": [
        "olivetti.target"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ugNjC9c01cCL"
      },
      "source": [
        "*Exercise: Then split it into a training set, a validation set, and a test set (note that the dataset is already scaled between 0 and 1). Since the dataset is quite small, you probably want to use stratified sampling to ensure that there are the same number of images per person in each set.*"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tkxhjVVH1cCM"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import StratifiedShuffleSplit\n",
        "\n",
        "strat_split = StratifiedShuffleSplit(n_splits=1, test_size=40, random_state=42)\n",
        "train_valid_idx, test_idx = next(strat_split.split(olivetti.data,\n",
        "                                                   olivetti.target))\n",
        "X_train_valid = olivetti.data[train_valid_idx]\n",
        "y_train_valid = olivetti.target[train_valid_idx]\n",
        "X_test = olivetti.data[test_idx]\n",
        "y_test = olivetti.target[test_idx]\n",
        "\n",
        "strat_split = StratifiedShuffleSplit(n_splits=1, test_size=80, random_state=43)\n",
        "train_idx, valid_idx = next(strat_split.split(X_train_valid, y_train_valid))\n",
        "X_train = X_train_valid[train_idx]\n",
        "y_train = y_train_valid[train_idx]\n",
        "X_valid = X_train_valid[valid_idx]\n",
        "y_valid = y_train_valid[valid_idx]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kv9obpdf1cCM"
      },
      "outputs": [],
      "source": [
        "print(X_train.shape, y_train.shape)\n",
        "print(X_valid.shape, y_valid.shape)\n",
        "print(X_test.shape, y_test.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rN_jllw11cCM"
      },
      "source": [
        "*Exercise: Next, cluster the images using K-Means, and ensure that you have a good number of clusters (using one of the techniques discussed in this chapter).*"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tX8e6orM1cCM"
      },
      "outputs": [],
      "source": [
        "from sklearn.cluster import KMeans\n",
        "\n",
        "k_range = range(5, 150, 5)\n",
        "kmeans_per_k = []\n",
        "for k in k_range:\n",
        "    print(f\"k={k}\")\n",
        "    kmeans = KMeans(n_clusters=k, random_state=42)\n",
        "    kmeans.fit(X_train)\n",
        "    kmeans_per_k.append(kmeans)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qQ_MWNpW1cCN"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import silhouette_score\n",
        "\n",
        "silhouette_scores = [silhouette_score(X_train, model.labels_)\n",
        "                     for model in kmeans_per_k]\n",
        "best_index = np.argmax(silhouette_scores)\n",
        "best_k = k_range[best_index]\n",
        "best_score = silhouette_scores[best_index]\n",
        "\n",
        "plt.figure(figsize=(8, 3))\n",
        "plt.plot(k_range, silhouette_scores, \"bo-\")\n",
        "plt.xlabel(\"$k$\")\n",
        "plt.ylabel(\"Silhouette score\")\n",
        "plt.plot(best_k, best_score, \"rs\")\n",
        "plt.grid()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "INPzrrET1cCN"
      },
      "outputs": [],
      "source": [
        "best_k"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gwo0Lrmw1cCN"
      },
      "source": [
        "It looks like the best number of clusters is quite high, at 120. You might have expected it to be 40, since there are 40 different people on the pictures. However, the same person may look quite different on different pictures (e.g., with or without glasses, or simply shifted left or right)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vm4eVFBh1cCN"
      },
      "outputs": [],
      "source": [
        "inertias = [model.inertia_ for model in kmeans_per_k]\n",
        "best_inertia = inertias[best_index]\n",
        "\n",
        "plt.figure(figsize=(8, 3.5))\n",
        "plt.plot(k_range, inertias, \"bo-\")\n",
        "plt.xlabel(\"$k$\")\n",
        "plt.ylabel(\"Inertia\")\n",
        "plt.plot(best_k, best_inertia, \"rs\")\n",
        "plt.grid()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Zj9eHBSA1cCN"
      },
      "source": [
        "The optimal number of clusters is not clear on this inertia diagram, as there is no obvious elbow, so let's stick with k=120."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XrRfqbRG1cCN"
      },
      "outputs": [],
      "source": [
        "best_model = kmeans_per_k[best_index]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oiUqJzlQ1cCN"
      },
      "source": [
        "*Exercise: Visualize the clusters: do you see similar faces in each cluster?*"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ISNKDDX71cCN"
      },
      "outputs": [],
      "source": [
        "def plot_faces(faces, labels, n_cols=5):\n",
        "    faces = faces.reshape(-1, 64, 64)\n",
        "    n_rows = (len(faces) - 1) // n_cols + 1\n",
        "    plt.figure(figsize=(n_cols, n_rows * 1.1))\n",
        "    for index, (face, label) in enumerate(zip(faces, labels)):\n",
        "        plt.subplot(n_rows, n_cols, index + 1)\n",
        "        plt.imshow(face, cmap=\"gray\")\n",
        "        plt.axis(\"off\")\n",
        "        plt.title(label)\n",
        "    plt.show()\n",
        "\n",
        "for cluster_id in np.unique(best_model.labels_):\n",
        "    print(\"Cluster\", cluster_id)\n",
        "    in_cluster = best_model.labels_==cluster_id\n",
        "    faces = X_train[in_cluster]\n",
        "    labels = y_train[in_cluster]\n",
        "    plot_faces(faces, labels)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SlLm1j3O1cCO"
      },
      "source": [
        "About 2 out of 3 clusters are useful: that is, they contain at least 2 pictures, all of the same person. However, the rest of the clusters have either one or more intruders, or they have just a single picture.\n",
        "\n",
        "Clustering images this way may be too imprecise to be directly useful when training a model (as we will see below), but it can be tremendously useful when labeling images in a new dataset: it will usually make labelling much faster."
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.6"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}